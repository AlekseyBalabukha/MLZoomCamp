{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BentoML\n",
    "production ready machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real business cases, when we finished in training, tuning and picking up the best model  \n",
    "How we can get to real people for example applying for a Loan etc.?  \n",
    "![](./pic/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One option is to create a web-service for the created model (for example with Flask) and upload it into Cloud to then interact with that service\n",
    "- But how to make sure that our service is reliable enough to handle not 10 requests per second but 100 or even 1000 depending on use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will see how to build ML models on scale:\n",
    "- build and deploy ML service\n",
    "- customize ML service to fit specific use case\n",
    "- Make service **production ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is production ready?\n",
    "1. Scalability\n",
    "2. Operationally efficiency\n",
    "    - being able to maintain service without spending too much time on that\n",
    "3. Repeatability (CI/CD)\n",
    "    - what if we need to update the model every week?\n",
    "4. Flexibility\n",
    "    - meet business requirements in changing conditions\n",
    "5. Resiliency\n",
    "    - we need to be able to easily get back to a stable version\n",
    "6. Easy to use-ity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bento - packing all the components of ML service into some sort of deployable unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to take Model that we selected in the previous module (Tree-based models), specifically XGBoost model and build service on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading and preparation:\n",
    "df = pd.read_csv('./data/CreditScoring.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "# map target variable:\n",
    "df['status'] = df['status'].map({\n",
    "    1: 'ok',\n",
    "    2: 'default',\n",
    "    0: 'unk'\n",
    "})\n",
    "# map other features:\n",
    "home_values = {\n",
    "    1: 'rent',\n",
    "    2: 'owner',\n",
    "    3: 'private',\n",
    "    4: 'ignore',\n",
    "    5: 'parents',\n",
    "    6: 'other',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['home'] = df['home'].map(home_values)\n",
    "# matrital:\n",
    "marital_values = {\n",
    "    1: 'single',\n",
    "    2: 'married',\n",
    "    3: 'widow',\n",
    "    4: 'separated',\n",
    "    5: 'divorced',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['marital'] = df['marital'].map(marital_values)\n",
    "# records:\n",
    "records_values = {\n",
    "    1: 'no',\n",
    "    2: 'yes',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['records'] = df['records'].map(records_values)\n",
    "#jobs:\n",
    "job_values = {\n",
    "    1: 'fixed',\n",
    "    2: 'partime',\n",
    "    3: 'freelance',\n",
    "    4: 'others',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['job'] = df['job'].map(job_values)\n",
    "\n",
    "for column in ['income', 'assets', 'debt']:\n",
    "    df[column] = df[column].replace(to_replace=99999999, value=np.nan)\n",
    "df = df.fillna(0)\n",
    "df = df[df['status'] != 'unk'].reset_index(drop=True)\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=11)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=11)\n",
    "#reset_indexes:\n",
    "df_full_train = df_full_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# convert into binary format:\n",
    "df_full_train['status'] = (df_full_train['status'] == 'default').astype(int)\n",
    "df_train['status'] = (df_train['status'] == 'default').astype(int)\n",
    "df_val['status'] = (df_val['status'] == 'default').astype(int)\n",
    "df_test['status'] = (df_test['status'] == 'default').astype(int)\n",
    "\n",
    "#assign target variables separately:\n",
    "y_full_train = df_full_train['status'].values\n",
    "y_train = df_train['status'].values\n",
    "y_val = df_val['status'].values\n",
    "y_test = df_test['status'].values\n",
    "\n",
    "# remove target from dataset:\n",
    "del df_full_train['status']\n",
    "del df_train['status']\n",
    "del df_val['status']\n",
    "del df_test['status']\n",
    "\n",
    "# turn data into Dictionaries to use One-hot encoding later\n",
    "train_dicts = df_train.to_dict(orient='records')\n",
    "val_dicts = df_val.to_dict(orient='records')\n",
    "test_dicts = df_test.to_dict(orient='records')\n",
    "dicts_full_train = df_full_train.to_dict(orient='records')\n",
    "\n",
    "# train DictVectorizer:\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dicts)\n",
    "X_train = dv.transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)\n",
    "X_test = dv.transform(test_dicts)\n",
    "X_full_train = dv.transform(dicts_full_train)\n",
    "\n",
    "#matrix for xgboost\n",
    "dfulltrain = xgb.DMatrix(X_full_train, label=y_full_train)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score = 0.8324067738624701\n"
     ]
    }
   ],
   "source": [
    "# xgboost:\n",
    "xgb_params = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'nthread': 8,\n",
    "    'seed': 1,\n",
    "    'verbosity': 1,\n",
    "}\n",
    "model = xgb.train(xgb_params, dfulltrain, num_boost_round=175) # final model\n",
    "y_pred = model.predict(dtest)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f'auc score = {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### How we can save and load this model later to be able to run api service on this model?  \n",
    "1. One possible approach is to Pickle the model into pickle file and then load it into Flask application\n",
    "    - The problem with that approach is that depending on the ML framework, there are might be specific things to do in order to save the model properly (even within different versions of framework they may recommend different ways of saving the model). Therefore, it is important to look into the documentation.\n",
    "    - Bento ML allows to use simple method to save model, which does necessary steps for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pip install bentoml*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"credit_risk_model:ypwmfnlaxwefgpcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\ypwmfnlaxwefgpcv\\\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                           custom_objects={\n",
    "                                \"dictVectorizer\": dv\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is done behind this 1-row code is:\n",
    "- we are going through the process of saving the model in the way that is recommended\n",
    "- we tag it with id, which is unique every time we call the method save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create service on our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a file *service.py*, with code for our service\n",
    "- to load the model, we can use special method of BentoML: *bentoml.MLFramework.get* (in our case: *bentoml.xgboost.get()*)\n",
    "    - this function gets: tag, that we just created,  (credit_risk_model:ysdfym3aecblspcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"credit_risk_model:2th4ozdaxgkbopcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\2th4ozdaxgkbopcv\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ref = bentoml.xgboost.get('credit_risk_model:latest')\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Runner(runnable_class=<class 'bentoml._internal.frameworks.xgboost.get_runnable.<locals>.XGBoostRunnable'>, runnable_init_params={}, name='credit_risk_model', models=[Model(tag=\"credit_risk_model:2th4ozdaxgkbopcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\2th4ozdaxgkbopcv\")], resource_config=None, runner_methods=[RunnerMethod(runner=..., name='predict', config=RunnableMethodConfig(batchable=False, batch_dim=(0, 0), input_spec=None, output_spec=None), max_batch_size=100, max_latency_ms=10000)], scheduling_strategy=<class 'bentoml._internal.runner.strategy.DefaultStrategy'>, _runner_handle=<bentoml._internal.runner.runner_handle.DummyRunnerHandle object at 0x0000023760800DD0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get access to that model:\n",
    "model_runner = model_ref.to_runner()\n",
    "model_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- runner - is BentoML abstraction for BentoML the model itself\n",
    "    - it allows us to scale model, separately from the rest of the service\n",
    "    - it is very useful for high performing scenarios\n",
    "    - but also it is a way to access the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bentoml.Service(name=\"credit_risk_classifier\", runners=[credit_risk_model])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create our service:\n",
    "svc = bentoml.Service('credit_risk_classifier', runners=[model_runner])\n",
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, to the Service we provide name of Service as well as list of models, which is useful if we have multiple models, BentoML would pack them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JSON' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#from bentoml.io import JSON\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# service endpoint:\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39m@svc\u001b[39m\u001b[39m.\u001b[39mapi(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mJSON(), output\u001b[39m=\u001b[39mJSON())\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify\u001b[39m(application_data):\n\u001b[0;32m      6\u001b[0m     prediction \u001b[39m=\u001b[39m model_runner\u001b[39m.\u001b[39mpredict\u001b[39m.\u001b[39mrun(application_data)\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'JSON' is not defined"
     ]
    }
   ],
   "source": [
    "#from bentoml.io import JSON\n",
    "\n",
    "# service endpoint:\n",
    "@svc.api(input=JSON(), output=JSON())\n",
    "def classify(application_data):\n",
    "    prediction = model_runner.predict.run(application_data)\n",
    "    return {\"status\": \"Approved\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the model has the exact same methods as original model had\n",
    "- the only difference is that instead if calling the predict directly, we need to use predict.run()\n",
    "- this allows us to run prediction in couple different ways (which will help us to improve scalability of our service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call bento ml serve:  \n",
    "- *bentoml serve service.py:svc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we running the service script we can see that it is running locally with port 3000:  \n",
    "![](./pic/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we paste this URL to the browser (http://localhost:3000)  \n",
    "we see Swager UI - automatically generated UI for open API spak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can test our service with sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\n",
    "    \"seniority\": 3,\n",
    "    \"home\": \"owner\",\n",
    "    \"time\": 36,\n",
    "    \"age\": 26,\n",
    "    \"marital\": \"single\",\n",
    "    \"records\": \"no\",\n",
    "    \"job\": \"freelance\",\n",
    "    \"expenses\": 35,\n",
    "    \"income\": 0.0,\n",
    "    \"assets\": 60000.0,\n",
    "    \"debt\": 3000.0,\n",
    "    \"amount\": 800,\n",
    "    \"price\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is an option \"try it out\" to test our service in Browser:  \n",
    "after we execute service we get an error:  \n",
    "  ![](./pic/4.png)  \n",
    "  ![](./pic/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error message says: Not supported type for data: 'dict'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this happens due to the fact that we did not pass dictionary data directly to our model  \n",
    "we used DictVectorizer to transform our data into array with 1-hot encoding for categorical variables (the result was array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in BentoML there is a special tool for using external modules like DictVectorizer for our services:\n",
    "- for that we can pass additional parameter *custom_objects* when we save our model with BentoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"credit_risk_model:s4s6rbdaxgsacpcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\s4s6rbdaxgsacpcv\\\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                           custom_objects={\n",
    "                                \"dictVectorizer\": dv\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have new tag, and we will use this tag, since now it has DictVectorizer in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Expecting data to be a DMatrix object, got: ', <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(vector)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m---> 15\u001b[0m classify(sample_data)\n",
      "Cell \u001b[1;32mIn [25], line 12\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m(application_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify\u001b[39m(application_data):\n\u001b[0;32m     11\u001b[0m     vector \u001b[39m=\u001b[39m dv\u001b[39m.\u001b[39mtransform(application_data)\n\u001b[1;32m---> 12\u001b[0m     prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(vector)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:2135\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[39m\"\"\"Predict with data.  The full model will be used unless `iteration_range` is specified,\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m \u001b[39mmeaning user have to either slice the model or use the ``best_iteration``\u001b[39;00m\n\u001b[0;32m   2054\u001b[0m \u001b[39mattribute to get prediction from best model returned from early stopping.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \n\u001b[0;32m   2133\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, DMatrix):\n\u001b[1;32m-> 2135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpecting data to be a DMatrix object, got: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(data))\n\u001b[0;32m   2136\u001b[0m \u001b[39mif\u001b[39;00m validate_features:\n\u001b[0;32m   2137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(data)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Expecting data to be a DMatrix object, got: ', <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "#from bentoml.io import JSON\n",
    "\n",
    "model_ref = bentoml.xgboost.get('credit_risk_model:latest')\n",
    "dv = model_ref.custom_objects['dictVectorizer']\n",
    "\n",
    "model_runner = model_ref.to_runner()\n",
    "\n",
    "svc = bentoml.Service(\"credit_risk_classifier\", runners=[model_runner])\n",
    "\n",
    "def classify(application_data):\n",
    "    vector = dv.transform(application_data)\n",
    "    prediction = model.predict(vector)\n",
    "    return {\"status\": \"Approved\"}\n",
    "\n",
    "classify(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every time we change the code of the service, we would have to reload terminal where it is running  \n",
    "to eliminate this we can use --reload option running service, so every time we change the code, it will automatically reload service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*bentoml serve service.py:svc --reload*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy BentoML Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the saved models with BentoML  \n",
    "- *bentoml models list*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get info about model by it's tag:  \n",
    "- *bentoml models get TAG*\n",
    "![](./pic/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bentoml saves various information about the model, including version of framework, which is very important, since we need to make sure that the framework on which the model was trained is the same one that is used for deployment to eliminate any inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Bento unit\n",
    "- we need to create a Bento file:\n",
    "    - bentofile.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3002970879.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    labels:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "service: \"service.py:svc\"\n",
    "labels:\n",
    "  owner: bentoml-team\n",
    "  project: gallery\n",
    "include:\n",
    "- \"*.py\"\n",
    "python:\n",
    "  packages:\n",
    "    - xgboost\n",
    "    - sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- service - entry point for bentoml serve\n",
    "- labels - anything which is important for business purposes to understand what this project is about, what is involved\n",
    "- include  \n",
    "- exclude \n",
    "    - these are 2 sections that can be used to help to organize the project, it is especially important \n",
    "    - it helps to keep our Bento lightweight with just what we need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that we simply call: \n",
    "- bentoml build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and bentoml will create a packed service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that if we look into the bentos folder, we can see what was created by BentpML for us:  \n",
    "![](./pic/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- even docker file was created automatically, however there is still an option to customize it\n",
    "- in python section - there are requirements for the specific version of frameworks\n",
    "- model - model itself (custom objects and metadata nad model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so thanks to BentoML we have all the things required for ML service in one place, so that they all can be then containerized and put in any environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a Docker image\n",
    "- bentoml containerize credit_risk_classifier:TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to look for all docker images that we build:\n",
    "- docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now when we have our docker image, we can run it:\n",
    "- docker run -it --rm -p 3000:3000 containerize credit_risk_classifier:mifjnuda4geoeaav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and when docker container is running we can communicate with that service on localhost:3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previously we were able to interact with service running from docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but what if we got input data with the following errors:\n",
    "- missing one feature\n",
    "- random name of the field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! the problem is that service is not actually fails: it gives us some usual results, and this might be even worse than fail, since we have no idea that our input data were corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- therefore, we want to Fail, if input data does not look right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the place where Pydantic library comes into play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to change our service python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseModel is the class of that library that we are going to extend to create a **Data Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditApplication(BaseModel):\n",
    "    seniority: int\n",
    "    home: str\n",
    "    age: int\n",
    "    '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditApplication(BaseModel):\n",
    "    seniority: int\n",
    "    home: str\n",
    "    time: int\n",
    "    age: int\n",
    "    marital: str\n",
    "    records: str\n",
    "    job: str\n",
    "    expenses: int\n",
    "    income: float\n",
    "    assets: float\n",
    "    debt: float\n",
    "    amount: int\n",
    "    price: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the way how we may make sure that our data is validated is by passing our data model to the JSON input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @svc.api(input=JSON(pydantic_model=CreditApplication), output=JSON())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and since it is no longer plain json object (now it is class CreditApplication), we now need to transform our object of CreditApplication class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, if we pass wrong data (missing values ect) we would get error:    \n",
    "which says that input data is wrong\n",
    "![](./pic/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performance Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will test service with high volume of traffic and\n",
    "- and then optimize service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pip install locust**\n",
    "this library can provide traffic to our service to test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
