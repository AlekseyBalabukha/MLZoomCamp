{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BentoML\n",
    "production ready machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real business cases, when we finished in training, tuning and picking up the best model  \n",
    "How we can get to real people for example applying for a Loan etc.?  \n",
    "![](./pic/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One option is to create a web-service for the created model (for example with Flask) and upload it into Cloud to then interact with that service\n",
    "- But how to make sure that our service is reliable enough to handle not 10 requests per second but 100 or even 1000 depending on use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will see how to build ML models on scale:\n",
    "- build and deploy ML service\n",
    "- customize ML service to fit specific use case\n",
    "- Make service **production ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is production ready?\n",
    "1. Scalability\n",
    "2. Operationally efficiency\n",
    "    - being able to maintain service without spending too much time on that\n",
    "3. Repeatability (CI/CD)\n",
    "    - what if we need to update the model every week?\n",
    "4. Flexibility\n",
    "    - meet business requirements in changing conditions\n",
    "5. Resiliency\n",
    "    - we need to be able to easily get back to a stable version\n",
    "6. Easy to use-ity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bento - packing all the components of ML service into some sort of deployable unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to take Model that we selected in the previous module (Tree-based models), specifically XGBoost model and build service on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading and preparation:\n",
    "df = pd.read_csv('./data/CreditScoring.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "# map target variable:\n",
    "df['status'] = df['status'].map({\n",
    "    1: 'ok',\n",
    "    2: 'default',\n",
    "    0: 'unk'\n",
    "})\n",
    "# map other features:\n",
    "home_values = {\n",
    "    1: 'rent',\n",
    "    2: 'owner',\n",
    "    3: 'private',\n",
    "    4: 'ignore',\n",
    "    5: 'parents',\n",
    "    6: 'other',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['home'] = df['home'].map(home_values)\n",
    "# matrital:\n",
    "marital_values = {\n",
    "    1: 'single',\n",
    "    2: 'married',\n",
    "    3: 'widow',\n",
    "    4: 'separated',\n",
    "    5: 'divorced',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['marital'] = df['marital'].map(marital_values)\n",
    "# records:\n",
    "records_values = {\n",
    "    1: 'no',\n",
    "    2: 'yes',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['records'] = df['records'].map(records_values)\n",
    "#jobs:\n",
    "job_values = {\n",
    "    1: 'fixed',\n",
    "    2: 'partime',\n",
    "    3: 'freelance',\n",
    "    4: 'others',\n",
    "    0: 'unk'\n",
    "}\n",
    "df['job'] = df['job'].map(job_values)\n",
    "\n",
    "for column in ['income', 'assets', 'debt']:\n",
    "    df[column] = df[column].replace(to_replace=99999999, value=np.nan)\n",
    "df = df.fillna(0)\n",
    "df = df[df['status'] != 'unk'].reset_index(drop=True)\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=11)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=11)\n",
    "#reset_indexes:\n",
    "df_full_train = df_full_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# convert into binary format:\n",
    "df_full_train['status'] = (df_full_train['status'] == 'default').astype(int)\n",
    "df_train['status'] = (df_train['status'] == 'default').astype(int)\n",
    "df_val['status'] = (df_val['status'] == 'default').astype(int)\n",
    "df_test['status'] = (df_test['status'] == 'default').astype(int)\n",
    "\n",
    "#assign target variables separately:\n",
    "y_full_train = df_full_train['status'].values\n",
    "y_train = df_train['status'].values\n",
    "y_val = df_val['status'].values\n",
    "y_test = df_test['status'].values\n",
    "\n",
    "# remove target from dataset:\n",
    "del df_full_train['status']\n",
    "del df_train['status']\n",
    "del df_val['status']\n",
    "del df_test['status']\n",
    "\n",
    "# turn data into Dictionaries to use One-hot encoding later\n",
    "train_dicts = df_train.to_dict(orient='records')\n",
    "val_dicts = df_val.to_dict(orient='records')\n",
    "test_dicts = df_test.to_dict(orient='records')\n",
    "dicts_full_train = df_full_train.to_dict(orient='records')\n",
    "\n",
    "# train DictVectorizer:\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dicts)\n",
    "X_train = dv.transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)\n",
    "X_test = dv.transform(test_dicts)\n",
    "X_full_train = dv.transform(dicts_full_train)\n",
    "\n",
    "#matrix for xgboost\n",
    "dfulltrain = xgb.DMatrix(X_full_train, label=y_full_train)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score = 0.8324067738624701\n"
     ]
    }
   ],
   "source": [
    "# xgboost:\n",
    "xgb_params = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'nthread': 8,\n",
    "    'seed': 1,\n",
    "    'verbosity': 1,\n",
    "}\n",
    "model = xgb.train(xgb_params, dfulltrain, num_boost_round=175) # final model\n",
    "y_pred = model.predict(dtest)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f'auc score = {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### How we can save and load this model later to be able to run api service on this model?  \n",
    "1. One possible approach is to Pickle the model into pickle file and then load it into Flask application\n",
    "    - The problem with that approach is that depending on the ML framework, there are might be specific things to do in order to save the model properly (even within different versions of framework they may recommend different ways of saving the model). Therefore, it is important to look into the documentation.\n",
    "    - Bento ML allows to use simple method to save model, which does necessary steps for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pip install bentoml*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"credit_risk_model:ypwmfnlaxwefgpcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\ypwmfnlaxwefgpcv\\\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                           custom_objects={\n",
    "                                \"dictVectorizer\": dv\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is done behind this 1-row code is:\n",
    "- we are going through the process of saving the model in the way that is recommended\n",
    "- we tag it with id, which is unique every time we call the method save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create service on our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a file *service.py*, with code for our service\n",
    "- to load the model, we can use special method of BentoML: *bentoml.MLFramework.get* (in our case: *bentoml.xgboost.get()*)\n",
    "    - this function gets: tag, that we just created,  (credit_risk_model:ysdfym3aecblspcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"credit_risk_model:2th4ozdaxgkbopcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\2th4ozdaxgkbopcv\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ref = bentoml.xgboost.get('credit_risk_model:latest')\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Runner(runnable_class=<class 'bentoml._internal.frameworks.xgboost.get_runnable.<locals>.XGBoostRunnable'>, runnable_init_params={}, name='credit_risk_model', models=[Model(tag=\"credit_risk_model:2th4ozdaxgkbopcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\2th4ozdaxgkbopcv\")], resource_config=None, runner_methods=[RunnerMethod(runner=..., name='predict', config=RunnableMethodConfig(batchable=False, batch_dim=(0, 0), input_spec=None, output_spec=None), max_batch_size=100, max_latency_ms=10000)], scheduling_strategy=<class 'bentoml._internal.runner.strategy.DefaultStrategy'>, _runner_handle=<bentoml._internal.runner.runner_handle.DummyRunnerHandle object at 0x0000023760800DD0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get access to that model:\n",
    "model_runner = model_ref.to_runner()\n",
    "model_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- runner - is BentoML abstraction for BentoML the model itself\n",
    "    - it allows us to scale model, separately from the rest of the service\n",
    "    - it is very useful for high performing scenarios\n",
    "    - but also it is a way to access the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bentoml.Service(name=\"credit_risk_classifier\", runners=[credit_risk_model])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create our service:\n",
    "svc = bentoml.Service('credit_risk_classifier', runners=[model_runner])\n",
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, to the Service we provide name of Service as well as list of models, which is useful if we have multiple models, BentoML would pack them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JSON' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#from bentoml.io import JSON\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# service endpoint:\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39m@svc\u001b[39m\u001b[39m.\u001b[39mapi(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mJSON(), output\u001b[39m=\u001b[39mJSON())\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify\u001b[39m(application_data):\n\u001b[0;32m      6\u001b[0m     prediction \u001b[39m=\u001b[39m model_runner\u001b[39m.\u001b[39mpredict\u001b[39m.\u001b[39mrun(application_data)\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'JSON' is not defined"
     ]
    }
   ],
   "source": [
    "#from bentoml.io import JSON\n",
    "\n",
    "# service endpoint:\n",
    "@svc.api(input=JSON(), output=JSON())\n",
    "def classify(application_data):\n",
    "    prediction = model_runner.predict.run(application_data)\n",
    "    return {\"status\": \"Approved\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the model has the exact same methods as original model had\n",
    "- the only difference is that instead if calling the predict directly, we need to use predict.run()\n",
    "- this allows us to run prediction in couple different ways (which will help us to improve scalability of our service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call bento ml serve:  \n",
    "- *bentoml serve service.py:svc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we running the service script we can see that it is running locally with port 3000:  \n",
    "![](./pic/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we paste this URL to the browser (http://localhost:3000)  \n",
    "we see Swager UI - automatically generated UI for open API spak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can test our service with sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\n",
    "    \"seniority\": 3,\n",
    "    \"home\": \"owner\",\n",
    "    \"time\": 36,\n",
    "    \"age\": 26,\n",
    "    \"marital\": \"single\",\n",
    "    \"records\": \"no\",\n",
    "    \"job\": \"freelance\",\n",
    "    \"expenses\": 35,\n",
    "    \"income\": 0.0,\n",
    "    \"assets\": 60000.0,\n",
    "    \"debt\": 3000.0,\n",
    "    \"amount\": 800,\n",
    "    \"price\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is an option \"try it out\" to test our service in Browser:  \n",
    "after we execute service we get an error:  \n",
    "  ![](./pic/4.png)  \n",
    "  ![](./pic/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error message says: Not supported type for data: 'dict'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this happens due to the fact that we did not pass dictionary data directly to our model  \n",
    "we used DictVectorizer to transform our data into array with 1-hot encoding for categorical variables (the result was array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in BentoML there is a special tool for using external modules like DictVectorizer for our services:\n",
    "- for that we can pass additional parameter *custom_objects* when we save our model with BentoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"credit_risk_model:s4s6rbdaxgsacpcv\", path=\"C:\\Users\\dein5\\bentoml\\models\\credit_risk_model\\s4s6rbdaxgsacpcv\\\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                           custom_objects={\n",
    "                                \"dictVectorizer\": dv\n",
    "                           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have new tag, and we will use this tag, since now it has DictVectorizer in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Expecting data to be a DMatrix object, got: ', <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(vector)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m---> 15\u001b[0m classify(sample_data)\n",
      "Cell \u001b[1;32mIn [25], line 12\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m(application_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify\u001b[39m(application_data):\n\u001b[0;32m     11\u001b[0m     vector \u001b[39m=\u001b[39m dv\u001b[39m.\u001b[39mtransform(application_data)\n\u001b[1;32m---> 12\u001b[0m     prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(vector)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mApproved\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:2135\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[39m\"\"\"Predict with data.  The full model will be used unless `iteration_range` is specified,\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m \u001b[39mmeaning user have to either slice the model or use the ``best_iteration``\u001b[39;00m\n\u001b[0;32m   2054\u001b[0m \u001b[39mattribute to get prediction from best model returned from early stopping.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \n\u001b[0;32m   2133\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, DMatrix):\n\u001b[1;32m-> 2135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpecting data to be a DMatrix object, got: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(data))\n\u001b[0;32m   2136\u001b[0m \u001b[39mif\u001b[39;00m validate_features:\n\u001b[0;32m   2137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(data)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Expecting data to be a DMatrix object, got: ', <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "#from bentoml.io import JSON\n",
    "\n",
    "model_ref = bentoml.xgboost.get('credit_risk_model:latest')\n",
    "dv = model_ref.custom_objects['dictVectorizer']\n",
    "\n",
    "model_runner = model_ref.to_runner()\n",
    "\n",
    "svc = bentoml.Service(\"credit_risk_classifier\", runners=[model_runner])\n",
    "\n",
    "def classify(application_data):\n",
    "    vector = dv.transform(application_data)\n",
    "    prediction = model.predict(vector)\n",
    "    return {\"status\": \"Approved\"}\n",
    "\n",
    "classify(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every time we change the code of the service, we would have to reload terminal where it is running  \n",
    "to eliminate this we can use --reload option running service, so every time we change the code, it will automatically reload service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bentoml Serve  \n",
    "- **bentoml serve service.py:svc --reload**  \n",
    "\n",
    "- if port is busy: \n",
    "    - sudo kill -9 $(sudo lsof -t -i:3000)\n",
    "    - fuser -k 3000/tcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy BentoML Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the saved models with BentoML  \n",
    "- *bentoml models list*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get info about model by it's tag:  \n",
    "- *bentoml models get TAG*\n",
    "![](./pic/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bentoml saves various information about the model, including version of framework, which is very important, since we need to make sure that the framework on which the model was trained is the same one that is used for deployment to eliminate any inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Bento unit\n",
    "- we need to create a Bento file:\n",
    "    - bentofile.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3002970879.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    labels:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "service: \"service.py:svc\"\n",
    "labels:\n",
    "  owner: bentoml-team\n",
    "  project: gallery\n",
    "include:\n",
    "- \"*.py\"\n",
    "python:\n",
    "  packages:\n",
    "    - xgboost\n",
    "    - sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- service - entry point for bentoml serve\n",
    "- labels - anything which is important for business purposes to understand what this project is about, what is involved\n",
    "- include  \n",
    "- exclude \n",
    "    - these are 2 sections that can be used to help to organize the project, it is especially important \n",
    "    - it helps to keep our Bento lightweight with just what we need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that we simply call: \n",
    "- bentoml build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and bentoml will create a packed service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after that if we look into the bentos folder, we can see what was created by BentpML for us:  \n",
    "![](./pic/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- even docker file was created automatically, however there is still an option to customize it\n",
    "- in python section - there are requirements for the specific version of frameworks\n",
    "- model - model itself (custom objects and metadata nad model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so thanks to BentoML we have all the things required for ML service in one place, so that they all can be then containerized and put in any environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a Docker image\n",
    "- bentoml containerize credit_risk_classifier:TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to look for all docker images that we build:\n",
    "- docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now when we have our docker image, we can run it:\n",
    "- docker run -it --rm -p 3000:3000 containerize credit_risk_classifier:mifjnuda4geoeaav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and when docker container is running we can communicate with that service on localhost:3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previously we were able to interact with service running from docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but what if we got input data with the following errors:\n",
    "- missing one feature\n",
    "- random name of the field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! the problem is that service is not actually fails: it gives us some usual results, and this might be even worse than fail, since we have no idea that our input data were corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- therefore, we want to Fail, if input data does not look right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the place where Pydantic library comes into play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to change our service python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseModel is the class of that library that we are going to extend to create a **Data Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditApplication(BaseModel):\n",
    "    seniority: int\n",
    "    home: str\n",
    "    age: int\n",
    "    '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditApplication(BaseModel):\n",
    "    seniority: int\n",
    "    home: str\n",
    "    time: int\n",
    "    age: int\n",
    "    marital: str\n",
    "    records: str\n",
    "    job: str\n",
    "    expenses: int\n",
    "    income: float\n",
    "    assets: float\n",
    "    debt: float\n",
    "    amount: int\n",
    "    price: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the way how we may make sure that our data is validated is by passing our data model to the JSON input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @svc.api(input=JSON(pydantic_model=CreditApplication), output=JSON())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and since it is no longer plain json object (now it is class CreditApplication), we now need to transform our object of CreditApplication class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, if we pass wrong data (missing values ect) we would get error:    \n",
    "which says that input data is wrong\n",
    "![](./pic/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performance Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will test service with high volume of traffic and\n",
    "- and then optimize service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pip install locust**\n",
    "this library can provide traffic to our service to test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to create so called *locustfile.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it contains sample of the data that we want to send   \n",
    "it is similar to swagger ui, but locust will send 100, 1000 and even more requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after creating locust file, we can run in terminal using one of these commands:\n",
    "- locust -H http://localhost:3000,  \n",
    "             in case if all requests failed then load client with\n",
    "- locust -H http://localhost:3000 -f locustfile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open browser at http://0.0.0.0:8089 (localhost:8089), adjust desired number of users and spawn\n",
    "        rate for the load test from the Web UI and start swarming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can choose how many users and how ofter (times per second) will make requests to out service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is also important to remember that the default BentoML setting for failure is if request takes longer than 1 second (it can be adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after increasing number of users = 1000 and frequency of requests = 10, on number of users = 810 we start to see that service starts to fail, therefore if we expect such high demand on our service, we should optimize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- async-await optimization  \n",
    "by default all requests in BentoML service are in que, meaning that BentoML service receives the request, services it and only after that comes to next request  \n",
    "what async allows service to do - process these requests in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to implement it we need to modify *service.py* file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add async before def classify  \n",
    "and add await before model_runner.predict  \n",
    "also change predict method from predict.run to predict.async_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@svc.api(input=JSON(pydantic_model=CreditApplication), output=JSON()) # decorate endpoint as in json format for input and output\n",
    "async def classify(credit_application):\n",
    "    application_data = credit_application.dict()\n",
    "    vector = dv.transform(application_data)\n",
    "    prediction = await model_runner.predict.async_run(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it allows to parallelize at endpoint level (same like in frameworks like FastAPI which allows to parallelize on endpoint level)\n",
    "- BentoML gives us an additional feature of being able to parallelize at the **inference level** by calling **async_run** rather that run  \n",
    "    - this means that we still can call our method (predict), but now we are able to call it in a different ways (async_run or just a normal run)\n",
    "    - it also allows us to fan out in more creative ways when we come to distributed on several machines services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after implementing async functionality, service did not fail even with 1000 users and 1000 requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, it is better to run traffic generator on the other machine than the one which serves ML Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Optimization - Fans out the processes for the service (**micro-batching**):  \n",
    "    - when we run our service, we create a Process, and therefore, only 1 CPU (Central Processing Unit) is able to run it\n",
    "    - to have advantage of using several CPUs we need to create several processes  \n",
    "    - we can replicate our service and run several processes at a time, so when the requests are coming they will be able to go to different CPU cores\n",
    "      \n",
    "      ![](./pic/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, there are several problems with this approach in ML:\n",
    "- if our model is big enough (GB) we can only copy till we run out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is much much more efficient to give several input to the model at the time  \n",
    "![](./pic/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we can combine requests into batches, and then send them into the model, we can have huge efficiency boost (micro-batching)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for doing this, we need to come to script, where we saved the model itself:  \n",
    "train_save_model.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                           custom_objects={\n",
    "                                \"DictVectorizer\": dv\n",
    "                           },\n",
    "                           signatures = { # models signatures for runner inference\n",
    "                            'predict': {\n",
    "                                'batchable': True,\n",
    "                                'batch_dim': 0 # 0 means BentoML will concatenate request\n",
    "                            }\n",
    "                           }\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can provide signatures features for save model function, where we can specify parameters of Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the new model, and change serve.py script to serve_batches.py (provide new tag for model: credit_risk_model:2xqrm5tfrkx3yaav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, to serve this model, we will need to use -- production flag, which tells machine that we want more then just 1 processors for our workers:  \n",
    "- **bentoml serve --production**  \n",
    "- bentoml serve service_async_batches.py:svc --reload --production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 2 most important parameters considering batching:\n",
    "- Max Batch Size, we can say: do not pack together more than 100 request\n",
    "- Max Latency - we can say do not wait longer than 5 ms (milliseconds), before sending request to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can configure these parameters by creating: *bentoconfiguration.yaml* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
