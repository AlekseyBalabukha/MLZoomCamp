{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the model from Session_4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.840 +- 0.008\n",
      "auc_score = 0.8572386167896259\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('churn_data.csv')\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "df.churn = (df.churn == 'yes').astype(int)\n",
    "\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "y_test = df_test['churn'].values\n",
    "\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "categorical = [\n",
    "    'gender',\n",
    "    'seniorcitizen',\n",
    "    'partner',\n",
    "    'dependents',\n",
    "    'phoneservice',\n",
    "    'multiplelines',\n",
    "    'internetservice',\n",
    "    'onlinesecurity',\n",
    "    'onlinebackup',\n",
    "    'deviceprotection',\n",
    "    'techsupport',\n",
    "    'streamingtv',\n",
    "    'streamingmovies',\n",
    "    'contract',\n",
    "    'paperlessbilling',\n",
    "    'paymentmethod',\n",
    "]\n",
    "\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "C = 1.0\n",
    "n_splits = 5\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    "\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f'auc_score = {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have model in notebook\n",
    "- we want to save it into file\n",
    "- then we want to load it into web-service\n",
    "- then we want use model on web-service which will interact with other services etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have model\n",
    "- we put it on web service (**FLASK** (framework for web-services creation in Python) for example)\n",
    "- we want to isolate dependencies on that service that they do not interfere with other services on our machine (environment for Python dependencies (**Pipenv**))\n",
    "- isolate system dependencies (**Docker**)\n",
    "- deploy this on cloud (AWS - Elastic Beanstalk for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use pickle to save model as python object:\n",
    "import pickle\n",
    "output_file = f'model_C={C}.bin'\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to write (w) into binary (b) file\n",
    "f_out = open('output_file', 'wb')\n",
    "pickle.dump(model, f_out)\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but now we saved only Model, however in Predict function, we also had DictVectorizer required, otherwise we would not be able to properly transform data from users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is why we will write Tuple (dv, model):\n",
    "f_out = open(output_file, 'wb')\n",
    "pickle.dump((dv, model), f_out) # dv added\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is easy to forget to close the file, so it is better to use with:\n",
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new customer:\n",
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363584152721566"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dv.transform([customer])\n",
    "\n",
    "model.predict_proba(X)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can put these pipeline into .py scripts (train.py and predict.py)  \n",
    "and use them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to put our model in some kind of Churn Service (Web-service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Web-service - a way of communicating between 2 devices via Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use our predict script as Web-service - create API for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our Service is running, we may try to pretend that we here is the Marketing Service sending Request to our Churn Service:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we try to access our predict Sevice from browser, we get an Error: \"Method Not Allowed\", since browser sends the Get request but, the only possible method we coded is POST,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore we need to POST using Pyhton library \"requests\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requests\n",
    "import requests\n",
    "# url is the one that we used creating Predict Web-service\n",
    "url = 'http://localhost:9696/predict'\n",
    "# customer in json format:\n",
    "customer = {\n",
    "    \"gender\": \"female\",\n",
    "    \"seniorcitizen\": 0,\n",
    "    \"partner\": \"yes\",\n",
    "    \"dependents\": \"no\",\n",
    "    \"phoneservice\": \"no\",\n",
    "    \"multiplelines\": \"no_phone_service\",\n",
    "    \"internetservice\": \"dsl\",\n",
    "    \"onlinesecurity\": \"no\",\n",
    "    \"onlinebackup\": \"yes\",\n",
    "    \"deviceprotection\": \"no\",\n",
    "    \"techsupport\": \"no\",\n",
    "    \"streamingtv\": \"no\",\n",
    "    \"streamingmovies\": \"no\",\n",
    "    \"contract\": \"month-to-month\",\n",
    "    \"paperlessbilling\": \"yes\",\n",
    "    \"paymentmethod\": \"electronic_check\",\n",
    "    \"tenure\": 1,\n",
    "    \"monthlycharges\": 29.85,\n",
    "    \"totalcharges\": 29.85\n",
    "}\n",
    "\n",
    "# making POST request to the Predict Web Service:\n",
    "requests.post(url, json=customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got Error 500, in Terminal we can see:  \n",
    "\"TypeError: Object of type bool_ is not JSON serializable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"bool_\" is something that comes from numpy (response) from model, so we can convert bool response of our model into bool datatype explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(url, json=customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the response is 200, which means OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'churn': True, 'churn_probability': 0.6363584152721566}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get response content:\n",
    "response = requests.post(url, json=customer).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending promo email to customer: 999\n"
     ]
    }
   ],
   "source": [
    "# Marketing service app:\n",
    "if response['churn'] == True:\n",
    "    print(f'sending promo email to customer: 999')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reason for this warning is that we using plain (simple) Flask,  \n",
    "to eliminate this, we need to use WSGI server instead\n",
    "- WSGI - **Web Server Gateway Interface**. It is a specification that describes how a web server communicates with web applications, and how web applications can be chained together to process one request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use **gunicorn** (pip isnstall gunicorn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we need to tell gunicorn where our Flask app is:  \n",
    "- Linux: *gunicorn --bind 0.0.0.0:9696 predict:app*    \n",
    "\n",
    "- Windows:\n",
    "    - (The module fctnl is not available on Windows systems), so we can either install ubuntu terminal or use \"Waiters\":  \n",
    "    - *pip install waitress* \n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "waitress-serve --listen=0.0.0.0:9696 predict:app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'churn': True, 'churn_probability': 0.6363584152721566}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get response content:\n",
    "response = requests.post(url, json=customer).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python virtual environment: Pipenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if one project uses sklearn of one version and another project uses different version, they need to be able to leave on the same computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to install some library (for example sklearn) we type: *pip install sklearn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in that type computer looks at the $PATH direction to find what **pip** we are talking about, there then it is able to find path to the pip file, which then is running and requesting pypi.org for the latest or some specific version of lybrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for example, let us say that we need sklearn 0.24.2 for our churn prediction project, however some other project (for example Lead Scoring Service) requires other version of sklearn \n",
    "- Lead Scoring - the evaluation that potential client will become our client in future  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore, if we had these 2 projects on the same computer: we may have situation when some changes in later version of library influences the work of 1st project  \n",
    "so we need to find a way to keep them separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the solution is to use different virtual environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so these projects use different pythons, with their own packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are multiple ways of managing virtual environments:\n",
    "- classical python build in /venv\n",
    "- conda - what anaconda uses\n",
    "- pipenv - the one we will use but there are no big difference, the reason we will use it is officially recommended package management from python community\n",
    "- poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *pipenv* package manager, however they all are pretty similar\n",
    "- *pip install pipenv*\n",
    "- then if we want to install some package we would need to use pipenv:  \n",
    "    - *pipenv install pandas*\\\n",
    "- specify exact versions of required packages:\n",
    "    - *pipenv install numpy scikit-learn==0.24.2 flask*\n",
    "- after that **pipfile** is created with packages specified inside\n",
    "- so when our colleagues will want to work on this project, they can simply use *pipenv install* and all required packages will be installed\n",
    "- also pipfile.lock will be created which \"locks\" the packages exact versions to make sure they will be the same later as well, for all the dependencies\n",
    "- pipenv install will install that exact versions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run our service:\n",
    "- get into this environment (dependencies from this specific project)\n",
    "    - *pipenv shell** - launching subshell in virt env\n",
    "    - it shows which folder is used for storing this virt env\n",
    "    - we can see by *which gunicorn* that path to the unicorn library is different and much longer, since, virtual environment adds path before origional one to isolate our project\n",
    "    - therefore we can use gunicorn from this virtual env as ususal *gunicorn --bind 0.0.....*\n",
    "    - and then we can communicate with our web-service as usual\n",
    "    - to exit virt env: *CTRL+D* and then we can see that gunicorn is usual\n",
    "\n",
    "- to enter again **pipenv shell**\n",
    "    - gunicorn --bind ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! However, if project needs some specific version of System Library (not python dependency) virtual environment can not solve this issue  \n",
    "- like apt-get install \"...\"\n",
    "- for that we will need one more level of isolation (Docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Management: Docker  \n",
    "- why we need Docker\n",
    "- running a Python image with docker\n",
    "- dockerfile\n",
    "- building a docker image\n",
    "- running a docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker makes one step further from virtual environments, and lets us to isolate entire application from the rest system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "considering 2 different services, we can put them into isolated container (kind of special box on computer that pretends as it is the only one thing that is running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker image with python only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- go to docker hub for python docker container\n",
    "- we can choose from different versions: lets take *3.10.7-slim*\n",
    "- *docker run -it --rm python:3.10.7-slim* (--it - open terminal after, --rm -remove after exit, we do not want to keep it on machine) \n",
    "- sice we used -it we then get into python terminal, which runs in separate container and we can interact with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*docker run -it --rm -e  python:3.10.7-slim* (-e - entry point i.e. default command when we use docker run),  \n",
    "- since we used python image, we get into python terminal, but now:  \n",
    "    - *docker run -it --rm -entrypoint=bash python:3.10.7-slim* and we are in Linux bash terminal and can use apt-get install wget for example\n",
    "    - and now what we do here does not affect host outside system\n",
    "        - for example we can create folder mkdir and it will not affect outside system\n",
    "        - we can use pip to install libraries etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything we do inside Docker we can state in Docker file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FORM python:3.8.12-slim (docker image)\n",
    "\n",
    "- RUN pip install pipenv (since all depedencies)\n",
    "- WORKDIR /app (creates new folder and goes there)\n",
    "- COPY [\"Pipfile\", \"Pipfile.lock\", \"./\"] (copy pipfile to the workdirectory)\n",
    "- RUN pipenv install --system (create virtual environment), however we do not need virtual environment inside docker, since it is already isolated, therefore use *--system -deploy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *docker build -t zoomcamp-test .* \n",
    "- then docker runs al commands,\n",
    "- when it is finished building, we can run this image:  \n",
    "    - *docker run -it --rm --entrypoint=bash zoomcamp-test*\n",
    "- then we also need to copy model file (COPY [\"predict.py\", \"model_C=1.0.bin\", \"./\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after building an image and entering it, we can run gunicorn --bind ...   \n",
    "however, we can not yet listen to that web-service, since it is inside container  \n",
    "- we need to expose port of Docker container to allow Test.py to access this port (PORT MAPPING)  \n",
    "![](./pic/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can do EXPOSE inside docker container: *EXPOSE 9696*\n",
    "- then we want to run our model web service: *ENTRYPOINT [\"gunicorn\", \"--bind=0.0.0.0:9696\", \"predict.py\"]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After EXPOSE we also need to do MAPPING:\n",
    "    - docker run -it --rm -p 9696:9696 zoomcamp-test (-p means port and we say that 9696 is mapped with 9696)\n",
    "\n",
    "      \n",
    "*docker run -it --rm -p 9696:9696 zoomcamp-test*\n",
    "docker run -it --rm -p 9696:9696 card_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can successfully interact with web-service which runs inside docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! Now, when we have our web-service inside Container, it now no longer required to be hosted on our machine, it can now live in **Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment to the cloud: AWS Elastic Beanstalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the eb cli\n",
    "2. Running eb locally\n",
    "3. Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So there is cloud, which is running our docker container\n",
    "- We have marketing service, which communicates with churn service on cloud\n",
    "- There could be many different services which request data from Churn service, and Cloud can automatically understand that service in Scaling UP (duplicating more services)\n",
    "- If traffic is low it then can scale down automatically  \n",
    "![](./pic/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to install AWS ElasticBeans Stock only for current project (churn prediction service)\n",
    "    - therefore, we can use pipenv\n",
    "    - this is Dev dependency (something that we need only for deployment), therefore it is not something we want to have inside the container --dev  \n",
    "\n",
    "*pipenv install awsebcli --dev*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we go to virtual environment (pipenv shell)  \n",
    "- and there we can use *eb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eb init -p docker -r eu-west-1 churn-serving (p - platform, name-project)\n",
    "- then the Project was created /elstic... folder with config.yaml with information inside\n",
    "- eb local run --port 9696 -  now before we deploy we can test it locally\n",
    "- eb create churn-serving-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we need to change url from localhost to the Host specified by AWS  \n",
    "But it is also important to mention that this elastic bean service is open to the world, so we should be careful with it\n",
    "- probably we want to make sure that only computers from specific network only are able to access our service\n",
    "- for pet project it is ok, but we should not forget to turn it down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
