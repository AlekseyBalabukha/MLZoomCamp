{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the model from Session_4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.840 +- 0.008\n",
      "auc_score = 0.8572386167896259\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('churn_data.csv')\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "df.churn = (df.churn == 'yes').astype(int)\n",
    "\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "y_test = df_test['churn'].values\n",
    "\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "categorical = [\n",
    "    'gender',\n",
    "    'seniorcitizen',\n",
    "    'partner',\n",
    "    'dependents',\n",
    "    'phoneservice',\n",
    "    'multiplelines',\n",
    "    'internetservice',\n",
    "    'onlinesecurity',\n",
    "    'onlinebackup',\n",
    "    'deviceprotection',\n",
    "    'techsupport',\n",
    "    'streamingtv',\n",
    "    'streamingmovies',\n",
    "    'contract',\n",
    "    'paperlessbilling',\n",
    "    'paymentmethod',\n",
    "]\n",
    "\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "C = 1.0\n",
    "n_splits = 5\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    "\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(f'auc_score = {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have model in notebook\n",
    "- we want to save it into file\n",
    "- then we want to load it into web-service\n",
    "- then we want use model on web-service which will interact with other services etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have model\n",
    "- we put it on web service (**FLASK** (framework for web-services creation in Python) for example)\n",
    "- we want to isolate dependencies on that service that they do not interfere with other services on our machine (environment for Python dependencies (**Pipenv**))\n",
    "- isolate system dependencies (**Docker**)\n",
    "- deploy this on cloud (AWS - Elastic Beanstalk for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use pickle to save model as python object:\n",
    "import pickle\n",
    "output_file = f'model_C={C}.bin'\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to write (w) into binary (b) file\n",
    "f_out = open('output_file', 'wb')\n",
    "pickle.dump(model, f_out)\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but now we saved only Model, however in Predict function, we also had DictVectorizer required, otherwise we would not be able to properly transform data from users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is why we will write Tuple (dv, model):\n",
    "f_out = open(output_file, 'wb')\n",
    "pickle.dump((dv, model), f_out) # dv added\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is easy to forget to close the file, so it is better to use with:\n",
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new customer:\n",
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363584152721566"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dv.transform([customer])\n",
    "\n",
    "model.predict_proba(X)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can put these pipeline into .py scripts (train.py and predict.py)  \n",
    "and use them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to put our model in some kind of Churn Service (Web-service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Web-service - a way of communicating between 2 devices via Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use our predict script as Web-service - create API for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our Service is running, we may try to pretend that we here is the Marketing Service sending Request to our Churn Service:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we try to access our predict Sevice from browser, we get an Error: \"Method Not Allowed\", since browser sends the Get request but, the only possible method we coded is POST,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore we need to POST using Pyhton library \"requests\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requests\n",
    "import requests\n",
    "# url is the one that we used creating Predict Web-service\n",
    "url = 'http://localhost:9696/predict'\n",
    "# customer in json format:\n",
    "customer = {\n",
    "    \"gender\": \"female\",\n",
    "    \"seniorcitizen\": 0,\n",
    "    \"partner\": \"yes\",\n",
    "    \"dependents\": \"no\",\n",
    "    \"phoneservice\": \"no\",\n",
    "    \"multiplelines\": \"no_phone_service\",\n",
    "    \"internetservice\": \"dsl\",\n",
    "    \"onlinesecurity\": \"no\",\n",
    "    \"onlinebackup\": \"yes\",\n",
    "    \"deviceprotection\": \"no\",\n",
    "    \"techsupport\": \"no\",\n",
    "    \"streamingtv\": \"no\",\n",
    "    \"streamingmovies\": \"no\",\n",
    "    \"contract\": \"month-to-month\",\n",
    "    \"paperlessbilling\": \"yes\",\n",
    "    \"paymentmethod\": \"electronic_check\",\n",
    "    \"tenure\": 1,\n",
    "    \"monthlycharges\": 29.85,\n",
    "    \"totalcharges\": 29.85\n",
    "}\n",
    "\n",
    "# making POST request to the Predict Web Service:\n",
    "requests.post(url, json=customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got Error 500, in Terminal we can see:  \n",
    "\"TypeError: Object of type bool_ is not JSON serializable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"bool_\" is something that comes from numpy (response) from model, so we can convert bool response of our model into bool datatype explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(url, json=customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the response is 200, which means OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'churn': True, 'churn_probability': 0.6363584152721566}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get response content:\n",
    "response = requests.post(url, json=customer).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending promo email to customer: 999\n"
     ]
    }
   ],
   "source": [
    "# Marketing service app:\n",
    "if response['churn'] == True:\n",
    "    print(f'sending promo email to customer: 999')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reason for this warning is that we using plain (simple) Flask,  \n",
    "to eliminate this, we need to use WSGI server instead\n",
    "- WSGI - **Web Server Gateway Interface**. It is a specification that describes how a web server communicates with web applications, and how web applications can be chained together to process one request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use **gunicorn** (pip isnstall gunicorn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we need to tell gunicorn where our Flask app is:  \n",
    "*gunicorn --bind 0.0.0.0:9696 predict:app*  \n",
    "(The module fctnl is not available on Windows systems), so we can either install ubuntu terminal or use \"Waiters\":  \n",
    "*pip install waitress* \n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "waitress-serve --listen=0.0.0.0:9696 predict:app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'churn': True, 'churn_probability': 0.6363584152721566}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get response content:\n",
    "response = requests.post(url, json=customer).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python virtual environment: Pipenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if one project uses sklearn of one version and another project uses different version, they need to be able to leave on the same computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to install some library (for example sklearn) we type: *pip install sklearn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in that type computer looks at the $PATH direction to find what **pip** we are talking about, there then it is able to find path to the pip file, which then is running and requesting pypi.org for the latest or some specific version of lybrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for example, let us say that we need sklearn 0.24.2 for our churn prediction project, however some other project (for example Lead Scoring Service) requires other version of sklearn \n",
    "- Lead Scoring - the evaluation that potential client will become our client in future  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore, if we had these 2 projects on the same computer: we may have situation when some changes in later version of library influences the work of 1st project  \n",
    "so we need to find a way to keep them separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the solution is to use different virtual environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so these projects use different pythons, with their own packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are multiple ways of managing virtual environments:\n",
    "- classical python build in /venv\n",
    "- conda - what anaconda uses\n",
    "- pipenv - the one we will use but there are no big difference, the reason we will use it is officially recommended package management from python community\n",
    "- poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
